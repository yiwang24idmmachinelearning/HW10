{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# HW10\n",
    "\n",
    "## Image/Audio Analysis and Classification with Clustering and PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/audio_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/HW04/raw/main/HW04_utils.pyc\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/HW06/raw/main/HW06_utils.pyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from os import listdir, path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from audio_utils import wav_to_list\n",
    "\n",
    "from data_utils import PCA, RandomForestClassifier, StandardScaler, SVC\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from data_utils import LFWUtils\n",
    "\n",
    "from image_utils import make_image, open_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Unlock\n",
    "\n",
    "Let's train a model to detect our face. We can think of this as a simpler version of one of the components inside something like the face ID software on our phones.\n",
    "\n",
    "We'll skip the face detection part, which is when we find faces in an image, and assume we can get cropped and aligned faces out of images or video streams. We'll look at face detection later in the semester.\n",
    "\n",
    "This is a slightly different kind of problem from the classification exercise we did in class, but the process is mostly the same.\n",
    "\n",
    "We will use a dataset with other people's faces, but in the end we are only interested on how well our model detects our face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Always Start with the Data\n",
    "\n",
    "The dataset we're using is inside `./data/images/lfw/cropped`. It's a subset of the [Labeled Faces in the Wild](https://vis-www.cs.umass.edu/lfw/) dataset.\n",
    "\n",
    "Take a look at the directory.\n",
    "\n",
    "What's there?\n",
    "\n",
    "How's the data organized and labeled?\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "Since we're not interested in generic classification, and measuring how we do on unlabeled data, this whole dataset is labeled, and we can read it into `train` and `test` subsets by calling the `train_test_split()` function of the `LFWUtils` class.\n",
    "\n",
    "This function takes an optional parameter that specifies what portion of the data should be used for the `test` dataset. We can start with the default value of $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Data\n",
    "\n",
    "Ok. Data is loaded.\n",
    "\n",
    "What's in the data? How is it actually organized?\n",
    "\n",
    "Take a look at the objects that were returned in each of the $2$ variables.\n",
    "\n",
    "How big are our datasets?\n",
    "\n",
    "Take a look at the `LABELS` and `L2I` members of the `LFWUtils` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: look at dataset objects. What's in them?\n",
    "# TODO: how big are them?\n",
    "# TODO: how many labels do they have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "\n",
    "We can open some random images to make sure the content of our datasets make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train[\"labels\"])\n",
    "\n",
    "for idx in [random.randint(0, train_size - 1) for cnt in range(10)]:\n",
    "  label_id = train[\"labels\"][idx]\n",
    "  print(\"id:\", label_id,\n",
    "        \"\\nlabel:\", LFWUtils.LABELS[label_id],\n",
    "        \"\\nfrom:\", train[\"files\"][idx])\n",
    "  display(make_image(train[\"pixels\"][idx], width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding your images\n",
    "\n",
    "Create a directory in the `dataset` directory for your images. Give it a one-word name, like your last name, your nyu id or your initials. For example, mine is called `tgh` and is located at: `./data/images/lfw/cropped/tgh`.\n",
    "\n",
    "Now, add between $20$ and $30$ images of your face to your directory. \n",
    "\n",
    "The images should be just like the ones that are already there for the other people:\n",
    "- $130$ pixels wide by \n",
    "- $170$ pixels tall\n",
    "- single-channel grayscale\n",
    "- jpeg format\n",
    "- named `label-number.jpg` (for example: `tgh-000.jpg`)\n",
    "\n",
    "Feel free to do this manually using Photoshop or any other image editing software, but the easiest way is to use this interface that automatically crops faces out of pictures and creates images in the correct format:\n",
    "\n",
    "[Face Align](https://huggingface.co/spaces/IDMNYU/9103H-2024F-face-align-gradio)\n",
    "\n",
    "It will also align the faces and put the eyes in a consistent location. There's even an option to capture pictures from a live camera stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA, Classification, etc etc etc\n",
    "\n",
    "Now that we have added our images to the dataset, let's train a classifier and see how well it performs on not just classification, but on recognizing our face.\n",
    "\n",
    "We can aim for an explained variance value of about $80\\%$, and adjust that later if we find necessary.\n",
    "\n",
    "Once we have the PCs for our training dataset in a `DataFrame` we can add a `label` column to it with the correct labels we have in `train[\"labels\"]`.\n",
    "\n",
    "We can also create a `DataFrame` for testing now by using the same `PCA` object to `transform()` the `test[\"pixels\"]` data.\n",
    "\n",
    "Since we won't train anything with the test dataset, it's ok to just keep the labels in `test[\"labels\"]` as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: reload the dataset, now with our images\n",
    "# TODO: create PCA, fit and transform train data\n",
    "# TODO: check PCA captured variance\n",
    "# TODO: prepare DataFrame for training (add label column)\n",
    "# TODO: create the test DataFrame by running PCA on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following cell to take a look at our images and their reconstructions.\n",
    "\n",
    "Run the following to look at the first $5$ images of a given label and their re-constructed versions.\n",
    "\n",
    "This assumes the `DataFrame` is called `train_df` and the `PCA` object is called `face_pca`. Adjust these if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change this label to your label to see how\n",
    "#       your images are being encoded\n",
    "awesome_label = \"watts\"\n",
    "awesome_id = LFWUtils.L2I[awesome_label]\n",
    "\n",
    "# this filters the DataFrame by our label\n",
    "awesome_df = train_df[train_df[\"label\"] == awesome_id]\n",
    "\n",
    "# reconstruct all images\n",
    "pca_pixels = face_pca.inverse_transform(awesome_df)\n",
    "\n",
    "# associate the original indexes with the reconstructed pca pixels\n",
    "idx_pca = zip(awesome_df.index, pca_pixels.values)\n",
    "\n",
    "# iterate through indexes and pca pixels\n",
    "for img_idx, img_pca in list(idx_pca)[:5]:\n",
    "  print(\"label:\", awesome_label,\n",
    "        \"\\nfrom:\", train[\"files\"][img_idx])\n",
    "  display(make_image(train[\"pixels\"][img_idx], width=130))\n",
    "  display(make_image(img_pca, width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this cell will look at some random images and their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train[\"labels\"])\n",
    "pca_pixels = face_pca.inverse_transform(train_df)\n",
    "\n",
    "for idx in [random.randint(0, train_size - 1) for cnt in range(10)]:\n",
    "  label_id = train[\"labels\"][idx]\n",
    "  print(\"id:\", label_id,\n",
    "        \"\\nlabel:\", LFWUtils.LABELS[label_id],\n",
    "        \"\\nfrom:\", train[\"files\"][idx])\n",
    "  display(make_image(train[\"pixels\"][idx], width=130))\n",
    "  display(make_image(pca_pixels.loc[idx], width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Do these make sense ? Do they look \"recognizable\" ? How do they change as a function of <code>n_components</code> ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to classifying..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create a classifier\n",
    "# TODO: separate input and output columns from the train DataFrame\n",
    "# TODO: train model using train data and labels\n",
    "# TODO: run prediction on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model with training data\n",
    "\n",
    "We can use `classification_error(real_labels, predicted_labels)` to measure the classification error and `display_confusion_matrix(real_labels, predicted_labels, LFWUtils.LABELS)` to show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure classification error\n",
    "print(classification_error(train[\"labels\"], train_predictions))\n",
    "\n",
    "# look at confusion matrix\n",
    "display_confusion_matrix(train[\"labels\"], train_predictions, LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "How does the confusion matrix look ? What does it mean ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `RandomForestClassifier` performs well on the training data, we can move on and run it on the `test` data.\n",
    "\n",
    "If not, there are a couple of things we can try:\n",
    "- adjust `PCA` parameters\n",
    "- add more images to our dataset\n",
    "- try a different type of classifier called `SVC`\n",
    "- adjust the ratio of the `train`/`test` split and increase the number of training samples (but not past $60$ / $40$)\n",
    "\n",
    "We don't want to `fit()` another `PCA` here, nor create another `RandomForestClassifier` object.\n",
    "\n",
    "We already have the `PCA` components we need for the test dataset, probably in a variable called `test_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: run prediction on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model with testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure classification error\n",
    "print(classification_error(test[\"labels\"], test_predictions))\n",
    "\n",
    "# look at confusion matrix\n",
    "display_confusion_matrix(test[\"labels\"], test_predictions, LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "How does THIS confusion matrix look ? What does it mean ? How does it perform for your pictures ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "Accuracy, which is the complement of our `classification_error` value, is the measurement that is optimized during the `RandomForestClassifier` training process.\n",
    "\n",
    "If we were training a regular classifier, we would look at `accuracy` (or `classification_error`) to determine if our model's performance is acceptable.\n",
    "\n",
    "Since we're working on a personal face recognition model, we don't really care about overall accuracy, but instead are more interested in the `precision` and `recall` values for the classification of our particular images.\n",
    "\n",
    "We don't want overall accuracy to be horrible, but we can be more specific in this case and be happy if the correct portion of our confusion matrix looks good.\n",
    "\n",
    "Calculate the `precision` and `recall` values for the classification of your images.\n",
    "\n",
    "You can use some scikit-learn functions for this, but might be faster to just take a look [HERE](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall) or our [WK10 notebook](https://github.com/DM-GY-9103-2024F-H/WK10/blob/main/WK10.ipynb) to see how to calculate these from the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: calculate precision\n",
    "# TODO: calculate recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "How is it performing for your images ? Which value, precision or recall, is higher ? What does that mean ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the following cell to see which classes have the highest `precision` and `recall` scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(LFWUtils.top_precision(test[\"labels\"], test_predictions, top=5))\n",
    "display(LFWUtils.top_recall(test[\"labels\"], test_predictions, top=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Did that analysis make you feel any better about your classifier ? Is your face in the top-5 precision classes ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">EDIT THIS CELL WITH ANSWER</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust\n",
    "\n",
    "Do the `precision` and `recall` values seem acceptable ?\n",
    "\n",
    "Which is more important in this case? (Keep in mind what a false positive might mean...)\n",
    "\n",
    "A couple of things to try to improve the model:\n",
    "- adjust `PCA` parameters\n",
    "- add more images to our dataset\n",
    "- try a different type of classifier called `SVC`\n",
    "- adjust the ratio of the `train`/`test` split and increase the number of training samples (but not past $60$ / $40$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Image/Audio Classification\n",
    "\n",
    "We're going to re-visit the classification exercises from `HW04` and `HW06`, but will increment them with `PCA` and Clustering.\n",
    "\n",
    "This exercise is a bit different though. In some ways it's the opposite of many of the other exercises we've done so far, but it's more representative of the type of work that goes into using real ML models in the wild.\n",
    "\n",
    "So far, the code for loading data into datasets has mostly been provided, and you had to implement the steps necessary to create, fit and evaluate a model.\n",
    "\n",
    "This time, the models are all set up, but you have to prepare the data that is necessary to fit/train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "\n",
    "Let's start by importing some classes and functions that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW04_utils import HW04Utils, function as get_audio_label\n",
    "from HW06_utils import HW06Utils, function as get_image_label\n",
    "from HW10_utils import AwesomeAudioClassifier, AwesomeImageClassifier\n",
    "\n",
    "AUDIO_PATH = \"./data/sounds/instruments\"\n",
    "IMAGE_PATH = \"./data/images/forests\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Models\n",
    "\n",
    "We have two `Awesome` models, one for audio classification, and one for image classification. They both use `StandardScaler` and `PCA` to prepare the data for an eventual `SVC` classifier. The image classifier also uses `KMeanClustering` to get a reduced palette of colors for the classification.\n",
    "\n",
    "They both inherit their interface from the same class: `AwesomeClassifier`. You can take a look inside `HW10_utils.py` for all the details, but this is the TL;DR:\n",
    "\n",
    "- `AwesomeAudioClassifier(pca_components)`: this constructor takes $1$ argument, the number of `PCA` components to use.\n",
    "- `AwesomeImageClassifier(pca_components, num_colors)`: this constructor takes $2$ arguments, the number of `PCA` components to use and the number of colors to use in clustering.\n",
    "- `AwesomeClassifier.fit(features, labels)`: this function takes two `DataFrames`, one for the features to use in training and the other with the encoded labels of the features.\n",
    "- `AwesomeClassifier.predict(features)`: once the model has been _fitted_ we can use this function to get predictions for our data.\n",
    "\n",
    "And that's it. All pre-processing, scaling, `PCA`ing, clustering, fitting, etc is done inside the classes.\n",
    "\n",
    "Once we have predictions we can use `classification_error(labels, predictions)` and `display_confusion_matrix(labels, predictions)` to evaluate our models.\n",
    "\n",
    "There's code for running these steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Audio and Image files are in the `data/sounds/instruments` and `data/images/forests` directories respectively.\n",
    "\n",
    "What you have to do is create `train` and `test` datasets with features and labels. These should be the output of calling the [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function with some `DataFrames`.\n",
    "\n",
    "There are a couple of ways of doing this:\n",
    "- `train_features, test_features, train_labels, test_labels = train_test_split(features_df, labels_df, test_size=0.5)`\n",
    "- OR\n",
    "- `train_df, test_df = train_test_split(data_df, test_size=0.5)`\n",
    "\n",
    "In the first option, the input features and labels are already separated into $2$ `DataFrames`, while in the second option there's one `DataFrame` that has both features and label columns. If you use the second option you have to split `train_df` and `test_df` into features and labels `DataFrames` yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Labels\n",
    "\n",
    "A possible strategy to get the labels for all files is:\n",
    "\n",
    "- As you're iterating through the files and reading their contents, the functions `get_audio_label()` and `get_image_label()` can be used to extract the correct label names from the filenames.\n",
    "- The `L2I` (label to id) object can then be used to encode the labels into numeric values\n",
    "  - `HW04Utils.L2I[get_audio_label(\"2122479.wav\")]` should return $1$, for `guitar`\n",
    "  - `HW06Utils.L2I[get_image_label(\"3150190.jpg\")]` should return $0$, for `florist`\n",
    "\n",
    "This should be enough for creating `DataFrame`s with labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Features\n",
    "\n",
    "This is the challenging part.\n",
    "\n",
    "Once you have opened the images/audios and extracted samples/pixels, you may try to create a `DataFrame` directly from those.\n",
    "\n",
    "It might seem like it works, but if you take a look at the `DataFrame` you'll see some `NaN` values in some of the columns, and if you send that to the model it will barf and complain about `PCA` not liking `NaN`s in the data.\n",
    "\n",
    "This happens because all of the audios and images have different sizes. Hoooray !!\n",
    "\n",
    "Welcome to Machine Learning. This is probably where most of the time in any ML project is spent: cleaning up data and making sure it has the right format, size and shape that a model expects.\n",
    "\n",
    "For this exercise it won't be too hard to fix these.\n",
    "\n",
    "Let's start with the audio files since they're one-dimensional, and once we have the audio modeling working we'll come back to the image files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Data\n",
    "\n",
    "We can start with a little bit of code from `HW04` to get all of the filenames that end in `.wav` inside a given directory.\n",
    "\n",
    "Then, we'll loop through the filenames, open them as lists of samples, and also save the labels associated with each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension for getting all of the filenames that end in \"wav\" inside the audio directory\n",
    "audio_files = [f for f in listdir(AUDIO_PATH) if f.endswith(\"wav\")]\n",
    "\n",
    "# To store samples and labels for each file\n",
    "audio_labels = []\n",
    "audio_samples = []\n",
    "\n",
    "# Iterate through filenames, open files, append samples and labels\n",
    "for fname in audio_files:\n",
    "  audio_labels.append(HW04Utils.L2I[get_audio_label(fname)])\n",
    "  audio_samples.append(wav_to_list(path.join(AUDIO_PATH, fname)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Audio Data\n",
    "\n",
    "We have the samples and labels. We can check what they look like and think of a way to make the list of samples more consistent.\n",
    "\n",
    "When we're done, they should all have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of lists\n",
    "print(len(audio_labels), len(audio_samples))\n",
    "\n",
    "# Length of samples for first 5 files\n",
    "for s in audio_samples[:5]:\n",
    "  print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: go through the list of samples and make their lengths consistent\n",
    "\n",
    "# TODO: create samples_df and audio_labels_df DataFrames for train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `samples_df` and `audio_labels_df` should look something like this:\n",
    "\n",
    "<img src=\"./imgs/samples_df.jpg\" height=\"200px\"/> and <img src=\"./imgs/audio_labels_df.jpg\" height=\"200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets\n",
    "\n",
    "Now we can run `train_test_split()` to create train and test `DataFrames` for our features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(samples_df, audio_labels_df, test_size=0.5, random_state=1010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Model\n",
    "\n",
    "Now that we have train and test `DataFrame`s with consistent rows, we can fit and evaluate our model.\n",
    "\n",
    "This cell creates a classifier model, fits it using the `train` dataset, and then makes predictions for both `train` and `test` datasets in order to calculate the classification error.\n",
    "\n",
    "We just have to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an audio classifier\n",
    "mAAC = AwesomeAudioClassifier(24)\n",
    "\n",
    "# Fit the classifier with training features and labels\n",
    "mAAC.fit(train_features, train_labels)\n",
    "\n",
    "# Predict labels for train and test data\n",
    "train_predictions = mAAC.predict(train_features)\n",
    "test_predictions = mAAC.predict(test_features)\n",
    "\n",
    "# Evaluate predictions\n",
    "print(classification_error(train_labels, train_predictions))\n",
    "print(classification_error(test_labels, test_predictions))\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=HW04Utils.INSTRUMENTS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=HW04Utils.INSTRUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data\n",
    "\n",
    "This is a bit trickier, but, again, we can start with a little bit of code from `HW06` to get all of the filenames that end in `.jpg` inside a given directory.\n",
    "\n",
    "Then, we'll loop through the filenames, open them as images, and also save the labels associated with each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: get all of the filenames that end in \"jpg\" inside the image directory\n",
    "\n",
    "# TODO: iterate through filenames, open files, store image objects and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Image Data\n",
    "\n",
    "We have the images and their labels. We can investigate what they look like and, before we extract their pixels, make the images more consistent.\n",
    "\n",
    "When we're done, they should all have the same dimensions.\n",
    "\n",
    "There are a couple of ways to achieve this:\n",
    "- Slice: chop off the list of pixels.\n",
    "- Crop: use the `image.crop()` function to cut the images.\n",
    "- Resize: use `image.resize()` to stretch/squeeze the images into specific shapes.\n",
    "\n",
    "Documentation for [`crop()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.crop) and [`resize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.resize).\n",
    "\n",
    "Take a look at a few images before picking a strategy and then take a look after to see what the chosen strategy does to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: look at characteristics/dimensions of the images\n",
    "\n",
    "# TODO: go through the list of images and make their dimensions consistent\n",
    "\n",
    "# TODO: look at some images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Image `DataFrames`\n",
    "\n",
    "Once we have images of similar sizes, we can put their pixels into a `DataFrame` called `pixels_df`, and create another `DataFrame` called `image_labels_df` with image label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create DataFrames pixels_df from image pixels and image_labels_df from the file labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pixels_df` and `image_labels_df` should look something like this:\n",
    "\n",
    "<img src=\"./imgs/pixels_df.jpg\" height=\"350px\"/> and <img src=\"./imgs/image_labels_df.jpg\" height=\"350px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train/test datasets\n",
    "\n",
    "Now we can run `train_test_split()` to create train and test `DataFrames` for our pixels and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: use train_test_split() to create train and test DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Image Model\n",
    "\n",
    "Now that we have train and test `DataFrame`s with consistent rows, we can fit and evaluate our model.\n",
    "\n",
    "The exact variable names might be different, but you should have $4$ `DataFrames` at this point:\n",
    "`train_features`, `test_features`, `train_labels`, and `test_labels`.\n",
    "\n",
    "These were either all returned by `train_test_split()`, or you manually split up the result of `train_test_split()` into `train` and `test` datasets.\n",
    "\n",
    "The following cell creates a classifier model, fits it using the `train` dataset, and then makes predictions for both `train` and `test` datasets in order to calculate the classification error.\n",
    "\n",
    "We just have to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an image classifier\n",
    "mAIC = AwesomeImageClassifier(64, 8)\n",
    "\n",
    "# Fit the classifier with training features and labels\n",
    "mAIC.fit(train_features, train_labels)\n",
    "\n",
    "# Predict labels for train and test data\n",
    "train_predictions = mAIC.predict(train_features)\n",
    "test_predictions = mAIC.predict(test_features)\n",
    "\n",
    "# Evaluate predictions\n",
    "print(classification_error(train_labels, train_predictions))\n",
    "print(classification_error(test_labels, test_predictions))\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=HW06Utils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=HW06Utils.LABELS)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.17 ('hf-model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
